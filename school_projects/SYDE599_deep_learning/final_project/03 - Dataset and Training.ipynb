{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Custom FOG dataset\n",
    "class FOGDataset(Dataset):\n",
    "    def __init__(self, data_files, transform=None, target_transform=None):\n",
    "        self.data_files = data_files\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = pd.read_csv(self.data_files[idx])\n",
    "        label = sample[\"labels\"].to_numpy()\n",
    "        label[label > 0] = 1.0\n",
    "        label[label < 0] = 0.0\n",
    "        sample = sample.drop([\"labels\", \"timestamp\"], axis = 1).to_numpy()\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return sample, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.75469007  1.3250399 ]\n",
      "(180501,)\n",
      "(180501,)\n",
      "(180501, 30)\n"
     ]
    }
   ],
   "source": [
    "train_dir = \"D:/SYDE599/train_fog_data/\"\n",
    "test_dir = \"D:/SYDE599/test_fog_data/\"\n",
    "train_files = glob.glob(train_dir + \"*.csv\")\n",
    "test_files = glob.glob(train_dir + \"*.csv\")\n",
    "\n",
    "train_data = pd.read_csv(train_files[0])\n",
    "classes = train_data[\"labels\"].unique()\n",
    "\n",
    "print(train_data[\"labels\"].unique())\n",
    "print(train_data[\"labels\"].shape)\n",
    "print(train_data[\"timestamp\"].shape)\n",
    "print(train_data.drop([\"labels\",\"timestamp\"], axis = 1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up datasets and dataloaders\n",
    "train_data = FOGDataset(train_files)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=1, shuffle=True)\n",
    "\n",
    "test_data = FOGDataset(test_files)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOG Model\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, input_features=30, input_shape=10, nc=10, do=0.5, kernel=3, padding='valid', bn2d=True, mp=3, pool='max', bs=1):\n",
    "        super(Network, self).__init__()\n",
    "        \n",
    "        # MODEL ARCHITECTURE\n",
    "        \n",
    "        # Enable or disable batchnorm2d layers\n",
    "        self.bn2d = True if bn2d==1 else False\n",
    "        \n",
    "        # Batch size\n",
    "        self.bs = bs\n",
    "\n",
    "        self.h1 = nn.Conv1d(input_features, nc, kernel_size=1, padding=padding)\n",
    "        self.mp1 = nn.MaxPool2d(mp)\n",
    "        self.h2 = nn.Conv1d(nc, int(nc/2), kernel_size=kernel, padding=padding)\n",
    "        #self.h1 = nn.Linear(window*input_features, 256)\n",
    "\n",
    "        self.h3 = nn.Linear(2490, 256)\n",
    "\n",
    "        self.h4 = nn.Linear(256, 64)\n",
    "        # self.h2 = nn.Linear(256, 128)\n",
    "        # self.h3 = nn.Linear(128, 64)\n",
    "\n",
    "        self.output = nn.Linear(64, 1)\n",
    "\n",
    "        # Dropout with dropout rate set by parameters\n",
    "        self.dropout = nn.Dropout(p=do)\n",
    "\n",
    "        \n",
    "        \n",
    "    # Forward pass of model\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.h1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.h2(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        x = self.h3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.h4(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return self.output(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "def train(model, train_loader, optimizer, epoch, window, batch_size=28):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    acc = 0\n",
    "    \n",
    "    dataset_size = 0\n",
    "\n",
    "    update_steps = 28\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Iterate through patient trials\n",
    "    for patient_idx, (inputs, targets) in enumerate(train_loader):\n",
    "            target_len = targets.shape[1]\n",
    "            num_samples  = target_len - window\n",
    "\n",
    "            num_batches = np.floor(num_samples/batch_size).astype(int)\n",
    "            batch_idx = np.arange(num_batches)\n",
    "            np.random.shuffle(batch_idx)\n",
    "\n",
    "            # Split trials into separate samples for the given kernel size\n",
    "            for b in np.arange(num_batches):\n",
    "                    batch = batch_idx[b]\n",
    "                    i = batch*batch_size\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # For batch\n",
    "                    sample_list = [inputs[:,i+k:i+k+window,:] for k in np.arange(batch_size)]\n",
    "                    target_list = [targets[:, i+k+window-1] for k in np.arange(batch_size)]\n",
    "                    sample = torch.Tensor(batch_size, window, inputs.shape[2])\n",
    "                    target = torch.Tensor(batch_size, 1)\n",
    "                    torch.cat(sample_list, out=sample)\n",
    "                    torch.cat(target_list, out=target)\n",
    "                    \n",
    "                    sample = np.transpose(sample, (0,2,1))\n",
    "\n",
    "                    # Run input through model\n",
    "                    output = model(sample.float())\n",
    "\n",
    "                    loss = nn.BCELoss()(nn.Sigmoid()(output[:,0].float()), target.float())\n",
    "                    total_loss += loss\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    predictions = output.argmax(dim=1, keepdim=True)\n",
    "                    correct += predictions.eq(target.view_as(predictions)).sum()\n",
    "\n",
    "\n",
    "                    # if b % 1000 == 0:\n",
    "                    #     print('Epoch: {} Patient: {}/{} Batch: {}/{} Training loss: {:.6f}'.format(\n",
    "                    #             epoch,\n",
    "                    #             patient_idx + 1,\n",
    "                    #             len(train_loader.dataset),\n",
    "                    #             b + 1,\n",
    "                    #             num_batches,\n",
    "                    #             loss))\n",
    "\n",
    "                    \n",
    "            dataset_size += target_len\n",
    "\n",
    "            print('Epoch: {} {}/{} Training loss: {:.6f}; Training accuracy: {:.1f}%'.format(\n",
    "                    epoch,\n",
    "                    patient_idx * len(inputs) + 1,\n",
    "                    len(train_loader.dataset),\n",
    "                    loss,\n",
    "                    100.* correct/dataset_size))\n",
    "\n",
    "            \n",
    "            \n",
    "    print(\"DATASET_SIZE: \" + str(dataset_size))\n",
    "    print('Training loss: {:.6f}; Training accuracy: {}/{} ({:.1f}%)\\n'.format(\n",
    "        total_loss / dataset_size,\n",
    "        correct,\n",
    "        dataset_size,\n",
    "        100. * correct/dataset_size))\n",
    "\n",
    "    return total_loss / dataset_size, 100. * acc\n",
    "\n",
    "# Test the model\n",
    "def test(model, test_loader, window):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    acc = 0\n",
    "    dataset_size = 0\n",
    "    with torch.no_grad():\n",
    "        for patient_idx, (inputs, targets) in test_loader:\n",
    "            target_len = targets.shape[1]\n",
    "            num_samples  = target_len - window\n",
    "            idx = np.arange(num_samples)\n",
    "            np.random.shuffle(idx)\n",
    "            for batch_idx in np.arange(num_samples):\n",
    "                i = idx[batch_idx]\n",
    "                sample = inputs[0, i:i+window, :]\n",
    "                sample = np.transpose(sample, (0,2,1))\n",
    "                target = targets[0, i+window]\n",
    "                output = model(sample.float())\n",
    "                loss += nn.BCELoss()(nn.Sigmoid()(output[:,0].float()), target.float())\n",
    "                predictions = output.argmax(dim=1, keepdim=True)\n",
    "                correct += predictions.eq(targets.view_as(predictions)).sum()\n",
    "\n",
    "            dataset_size += target_len  \n",
    "            acc = acc*(patient_idx/(patient_idx+1)) + (correct/num_samples)/(patient_idx + 1)\n",
    "\n",
    "    loss = loss / dataset_size\n",
    "    print(\"TEST SET SIZE: \" + str(dataset_size))\n",
    "    print('Test loss: {:.6f}; Test accuracy: {}/{} ({:.1f}%)\\n'.format(\n",
    "        loss,\n",
    "        correct,\n",
    "        len(test_loader.dataset),\n",
    "        100. * acc))\n",
    "    return loss, 100. * acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 1/48 Training loss: 0.000727; Training accuracy: 48.4%\n",
      "Epoch: 0 2/48 Training loss: 0.007105; Training accuracy: 67.5%\n",
      "Epoch: 0 3/48 Training loss: 0.002506; Training accuracy: 72.4%\n",
      "Epoch: 0 4/48 Training loss: 0.788487; Training accuracy: 68.7%\n",
      "Epoch: 0 5/48 Training loss: 0.000003; Training accuracy: 69.6%\n",
      "Epoch: 0 6/48 Training loss: 0.000000; Training accuracy: 70.5%\n",
      "Epoch: 0 7/48 Training loss: 0.000006; Training accuracy: 69.3%\n",
      "Epoch: 0 8/48 Training loss: 0.000000; Training accuracy: 69.6%\n",
      "Epoch: 0 9/48 Training loss: 0.009251; Training accuracy: 69.1%\n",
      "Epoch: 0 10/48 Training loss: 0.363805; Training accuracy: 56.4%\n",
      "Epoch: 0 11/48 Training loss: 0.568654; Training accuracy: 56.5%\n",
      "Epoch: 0 12/48 Training loss: 0.000000; Training accuracy: 56.8%\n",
      "Epoch: 0 13/48 Training loss: 0.000125; Training accuracy: 58.1%\n",
      "Epoch: 0 14/48 Training loss: 0.000000; Training accuracy: 58.4%\n",
      "Epoch: 0 15/48 Training loss: 0.036476; Training accuracy: 57.9%\n",
      "Epoch: 0 16/48 Training loss: 0.033939; Training accuracy: 60.3%\n",
      "Epoch: 0 17/48 Training loss: 0.000000; Training accuracy: 63.3%\n",
      "Epoch: 0 18/48 Training loss: 0.717662; Training accuracy: 63.1%\n",
      "Epoch: 0 19/48 Training loss: 0.078401; Training accuracy: 61.2%\n",
      "Epoch: 0 20/48 Training loss: 0.013025; Training accuracy: 61.2%\n",
      "Epoch: 0 21/48 Training loss: 0.015984; Training accuracy: 59.5%\n",
      "Epoch: 0 22/48 Training loss: 0.441796; Training accuracy: 59.7%\n",
      "Epoch: 0 23/48 Training loss: 0.000205; Training accuracy: 59.7%\n",
      "Epoch: 0 24/48 Training loss: 0.431385; Training accuracy: 59.8%\n",
      "Epoch: 0 25/48 Training loss: 0.544589; Training accuracy: 59.7%\n",
      "Epoch: 0 26/48 Training loss: 0.602520; Training accuracy: 59.6%\n",
      "Epoch: 0 27/48 Training loss: 0.000002; Training accuracy: 59.4%\n",
      "Epoch: 0 28/48 Training loss: 0.000000; Training accuracy: 61.2%\n",
      "Epoch: 0 29/48 Training loss: 0.000088; Training accuracy: 61.4%\n",
      "Epoch: 0 30/48 Training loss: 0.461674; Training accuracy: 61.0%\n",
      "Epoch: 0 31/48 Training loss: 0.023697; Training accuracy: 61.2%\n",
      "Epoch: 0 32/48 Training loss: 0.557322; Training accuracy: 61.3%\n",
      "Epoch: 0 33/48 Training loss: 0.041346; Training accuracy: 61.4%\n",
      "Epoch: 0 34/48 Training loss: 0.054696; Training accuracy: 60.7%\n",
      "Epoch: 0 35/48 Training loss: 0.000000; Training accuracy: 62.3%\n",
      "Epoch: 0 36/48 Training loss: 0.000175; Training accuracy: 52.9%\n",
      "Epoch: 0 37/48 Training loss: 1.039847; Training accuracy: 52.9%\n",
      "Epoch: 0 38/48 Training loss: 0.185432; Training accuracy: 52.7%\n",
      "Epoch: 0 39/48 Training loss: 0.000000; Training accuracy: 53.7%\n",
      "Epoch: 0 40/48 Training loss: 0.002416; Training accuracy: 54.8%\n",
      "Epoch: 0 41/48 Training loss: 0.088069; Training accuracy: 54.5%\n",
      "Epoch: 0 42/48 Training loss: 0.127888; Training accuracy: 54.8%\n",
      "Epoch: 0 43/48 Training loss: 0.065184; Training accuracy: 54.9%\n",
      "Epoch: 0 44/48 Training loss: 0.111276; Training accuracy: 54.9%\n",
      "Epoch: 0 45/48 Training loss: 0.101590; Training accuracy: 54.9%\n",
      "Epoch: 0 46/48 Training loss: 0.065437; Training accuracy: 54.9%\n",
      "Epoch: 0 47/48 Training loss: 0.425489; Training accuracy: 54.7%\n",
      "Epoch: 0 48/48 Training loss: 0.102525; Training accuracy: 54.6%\n",
      "DATASET_SIZE: 5111548\n",
      "Training loss: 0.011548; Training accuracy: 2788400/5111548 (54.6%)\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "test() takes 3 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Tia\\Documents\\University\\MASc\\F22\\SYDE599\\syde599\\Project\\03 - Dataset and Training.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 39>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Tia/Documents/University/MASc/F22/SYDE599/syde599/Project/03%20-%20Dataset%20and%20Training.ipynb#W5sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m window \u001b[39m=\u001b[39m \u001b[39m500\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Tia/Documents/University/MASc/F22/SYDE599/syde599/Project/03%20-%20Dataset%20and%20Training.ipynb#W5sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39m28\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Tia/Documents/University/MASc/F22/SYDE599/syde599/Project/03%20-%20Dataset%20and%20Training.ipynb#W5sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m model_acc \u001b[39m=\u001b[39m run(window\u001b[39m=\u001b[39;49mwindow, batch_size\u001b[39m=\u001b[39;49mbatch_size)\n",
      "\u001b[1;32mc:\\Users\\Tia\\Documents\\University\\MASc\\F22\\SYDE599\\syde599\\Project\\03 - Dataset and Training.ipynb Cell 6\u001b[0m in \u001b[0;36mrun\u001b[1;34m(window, batch_size)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Tia/Documents/University/MASc/F22/SYDE599/syde599/Project/03%20-%20Dataset%20and%20Training.ipynb#W5sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m train_loss\u001b[39m.\u001b[39mappend(tr_loss\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy())\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Tia/Documents/University/MASc/F22/SYDE599/syde599/Project/03%20-%20Dataset%20and%20Training.ipynb#W5sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m train_acc\u001b[39m.\u001b[39mappend(tr_acc)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Tia/Documents/University/MASc/F22/SYDE599/syde599/Project/03%20-%20Dataset%20and%20Training.ipynb#W5sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m test_loss, test_acc \u001b[39m=\u001b[39m test(model, test_loader, window, batch_size)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Tia/Documents/University/MASc/F22/SYDE599/syde599/Project/03%20-%20Dataset%20and%20Training.ipynb#W5sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m val_loss\u001b[39m.\u001b[39mappend(test_loss\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy())\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Tia/Documents/University/MASc/F22/SYDE599/syde599/Project/03%20-%20Dataset%20and%20Training.ipynb#W5sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m val_acc\u001b[39m.\u001b[39mappend(test_acc\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy())\n",
      "\u001b[1;31mTypeError\u001b[0m: test() takes 3 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "# Run the model\n",
    "def run(window=100, batch_size=28):\n",
    "    # Set up datasets and dataloaders\n",
    "    train_data = FOGDataset(train_files)\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=1, shuffle=True)\n",
    "\n",
    "    test_data = FOGDataset(test_files)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=1, shuffle=True)\n",
    "    \n",
    "    #model = Network(params['nodes'])\n",
    "    model = Network(input_shape=window)\n",
    "    model = model.float()\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    val_loss = []\n",
    "    val_acc = []\n",
    "\n",
    "    EPOCHS = 10\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    for epoch in range(0, EPOCHS):\n",
    "        tr_loss, tr_acc = train(model, train_loader, optimizer, epoch, window, batch_size)\n",
    "        train_loss.append(tr_loss.detach().numpy())\n",
    "        train_acc.append(tr_acc)\n",
    "\n",
    "        test_loss, test_acc = test(model, test_loader, window)\n",
    "        val_loss.append(test_loss.detach().numpy())\n",
    "        val_acc.append(test_acc.detach().numpy())\n",
    "\n",
    "    return val_acc[-1]\n",
    "\n",
    "window = 500\n",
    "batch_size = 28\n",
    "model_acc = run(window=window, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "model = Network(input_shape=window)\n",
    "\n",
    "# Print summary for model with optimized parameters\n",
    "summary(model, (1,100,30))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('deepenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "57b1a6f252fb2b7967a04b62771246b49ca108f8b90a576903f9d66029559da6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

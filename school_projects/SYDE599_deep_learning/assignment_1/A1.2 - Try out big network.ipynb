{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(2)\n",
    "\n",
    "A = np.random.randn(10, 2)\n",
    "b = np.random.randn(10)\n",
    "\n",
    "C = np.random.randn(10, 10)\n",
    "d = np.random.randn(10)\n",
    "\n",
    "E = np.random.randn(1, 10)\n",
    "\n",
    "y = np.random.randn(1)\n",
    "\n",
    "def mse_torch(y_hat, y):\n",
    "    return 0.5 * torch.sum((y_hat - y) ** 2)\n",
    "\n",
    "def mse_numpy(y_hat, y):\n",
    "    return 0.5 * np.sum((y_hat - y) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "At = torch.from_numpy(A)\n",
    "At.requires_grad = True\n",
    "bt = torch.from_numpy(b)\n",
    "bt.requires_grad = True\n",
    "Ct = torch.from_numpy(C)\n",
    "Ct.requires_grad = True\n",
    "dt = torch.from_numpy(d)\n",
    "dt.requires_grad = True\n",
    "Et = torch.from_numpy(E)\n",
    "Et.requires_grad = True\n",
    "\n",
    "xt = torch.from_numpy(x)\n",
    "yt = torch.from_numpy(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_torch(xt):\n",
    "    yt = At @ xt + bt\n",
    "    yt = F.relu(yt)\n",
    "    yt = Ct @ yt + dt\n",
    "    yt = F.relu(yt)\n",
    "    yt = Et @ yt\n",
    "    return yt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = mse_torch(model_torch(xt), yt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.0756, dtype=torch.float64, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.backward()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000, -0.0000],\n",
       "        [ 1.2418, -0.2984],\n",
       "        [-1.6198,  0.3893],\n",
       "        [ 4.3955, -1.0563],\n",
       "        [ 0.0000, -0.0000],\n",
       "        [-8.3285,  2.0014],\n",
       "        [ 0.9349, -0.2247],\n",
       "        [ 3.5032, -0.8419],\n",
       "        [ 0.0000, -0.0000],\n",
       "        [ 2.1107, -0.5072]], dtype=torch.float64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "At.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mul0 = A @ x\n",
    "add0 = mul0 + b\n",
    "relu0 = np.maximum(0, add0)\n",
    "\n",
    "mul1 = C @ relu0\n",
    "add1 = mul1 + d\n",
    "relu1 = np.maximum(0, add1)\n",
    "\n",
    "mul2 = E @ relu1\n",
    "y_hat = mul2\n",
    "\n",
    "loss = 0.5 * np.sum((y_hat - y) ** 2)\n",
    "delta_y = y_hat - y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.199294931170332"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last layer: y_hat = E @ mul2\n",
    "# y = f(a, x), df/dx = a, xd = df/da\n",
    "a_prime = E.T\n",
    "E_prime = mul2\n",
    "grad_E = np.outer(delta_y, E_prime)\n",
    "delta = a_prime @ delta_y\n",
    "\n",
    "# relu1 = relu(add1)\n",
    "a_prime = (add1 >= 0).astype(np.float32)\n",
    "delta = a_prime * delta\n",
    "\n",
    "# add1 = mul1 + d\n",
    "a_prime = np.ones_like(d)\n",
    "d_prime = np.ones_like(d)\n",
    "grad_d = (d_prime * delta)\n",
    "delta = a_prime * delta\n",
    "\n",
    "# mul1 = C @ relu0\n",
    "a_prime = C.T\n",
    "C_prime = relu0\n",
    "grad_C = np.outer(delta, C_prime)\n",
    "delta = a_prime @ delta\n",
    "\n",
    "# relu0 = relu(add0)\n",
    "a_prime = (add0 >= 0).astype(np.float32)\n",
    "delta = a_prime * delta\n",
    "\n",
    "# add0 = mul0 + b\n",
    "a_prime = np.ones_like(b)\n",
    "b_prime = np.ones_like(b)\n",
    "grad_b = (b_prime * delta)\n",
    "delta = a_prime * delta\n",
    "\n",
    "# mul0 = A @ x\n",
    "a_prime = A.T\n",
    "A_prime = x\n",
    "grad_A = np.outer(delta, A_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 10.3594, -17.6940,   0.0000,  21.0760,  -0.4013,   4.2122,   0.0000,\n",
       "          -9.1358,   0.0000,   0.0000], dtype=torch.float64),\n",
       " array([ 10.35944731, -17.69400177,   0.        ,  21.07600819,\n",
       "         -0.40127032,   4.21216019,  -0.        ,  -9.13577466,\n",
       "          0.        ,  -0.        ]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bt.grad, grad_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.6089,  0.1655, -1.1472,  0.5495, -0.6791, -1.0682, -1.8927, -0.5223,\n",
       "        -0.0524, -0.7498], dtype=torch.float64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bt.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each operation `y = f(x, w)`, we need to record:\n",
    "- The input, `x`\n",
    "- The gradient wrt to `x`, $\\frac{\\partial f}{\\partial x} = x'$\n",
    "- Whether `w` requires a gradient\n",
    "  - If it does, then record $\\frac{\\partial f}{\\partial w} = w'$\n",
    "\n",
    "During the backwards step, we loop through the tape and compute:\n",
    "- $\\delta_y = \\hat{y} - y$\n",
    "- $\\frac{\\partial L}{\\partial w} = w' * \\delta$\n",
    "- $\\delta_{l-1} = x' * \\delta_l$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"./data/assignment-one-test-parameters.pkl\", 'rb') as f:\n",
    "    d = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['w1', 'w2', 'w3', 'b1', 'b2', 'b3', 'inputs', 'targets'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 2)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['inputs'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass in weight sizes, activations, biases\n",
    "\n",
    "num_neurons []\n",
    "activations []\n",
    "bias []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model:\n",
    "    def __init__():\n",
    "        self.tape = []\n",
    "    \n",
    "    def forward():\n",
    "        z = self.matmul(x, A)\n",
    "        z = self.add(z, b)\n",
    "        z = self.relu(z)\n",
    "        return z\n",
    "    \n",
    "    def matmul(self, x, A):\n",
    "        self.tape.append({\n",
    "            'input0': x,\n",
    "            'input1': A,\n",
    "            'output': A @ x,\n",
    "            'function': 'matmul'})\n",
    "        return A @ x\n",
    "\n",
    "    def matmul_backward(self, x, A, delta):\n",
    "        x_prime = A.T\n",
    "        A_prime = x\n",
    "        grad_A = np.outer(delta, A_prime)\n",
    "        delta = x_prime @ delta\n",
    "        A.grad = grad_A\n",
    "        return delta\n",
    "\n",
    "    def add(self, x, b):\n",
    "        self.tape.append({\n",
    "            'input0': x,\n",
    "            'input1': b,\n",
    "            'function': 'add'\n",
    "        })\n",
    "    \n",
    "    def add_backward(self, x, b, delta):\n",
    "        a_prime = np.ones_like(b)\n",
    "        b_prime = np.ones_like(b)\n",
    "        grad_b = (b_prime * delta)\n",
    "        delta = a_prime * delta\n",
    "        return delta, grad_b\n",
    "        \n",
    "    \n",
    "    def relu(self, a):\n",
    "        self.tape.stick({\n",
    "            'input0': a,\n",
    "            'function': 'relu'\n",
    "        })\n",
    "\n",
    "\n",
    "    def backward():\n",
    "        delta = compute_delta(y_hat, y)\n",
    "        for item in reversed(self.tape):\n",
    "            # Get operation, inputs\n",
    "            # Compute df/dw and df/dx\n",
    "            # If we care about the parameter gradients, compute dL/dw\n",
    "            # Compute the delta\n",
    "            # Pass the delta to\n",
    "            delta, param_update = self.op_backward(a, b, delta)\n",
    "            self.gradients[\"A\"] = param_update\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model:\n",
    "    def __init__():\n",
    "        self.layers = [layer1, layer2, ...]\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            self.inputs.append(x)\n",
    "            self.outputs.append(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, y):\n",
    "        delta = compute_delta(y)\n",
    "        for layer in reversed(self.layers):\n",
    "            delta = layer.backward(delta)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('datasci')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fddaefb328894158b465f763a80d93613a8dda1c2f29f2bb5673421f61ac7a4f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

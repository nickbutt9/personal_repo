{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refer to this link for implementation of subclassing ndarray\n",
    "# https://numpy.org/doc/stable/user/basics.subclassing.html#slightly-more-realistic-example-attribute-added-to-existing-array\n",
    "\n",
    "class adarray(np.ndarray):\n",
    "    def __new__(cls, input_array, requires_grad=True):\n",
    "        obj = np.asarray(input_array).view(cls)\n",
    "        obj.requires_grad = requires_grad\n",
    "        return obj\n",
    "\n",
    "    def __array_finalize__(self, obj=None):\n",
    "        if obj is None: return\n",
    "        self.requires_grad = getattr(obj, \"requires_grad\", None)\n",
    "        self.grad = np.zeros_like(self.__array__())\n",
    "        \n",
    "    def zero_grad(self):\n",
    "        self.grad = np.zeros_like(self.__array__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NnOps:\n",
    "    \"\"\"\n",
    "    Implements AD for certain neural network operations matmul, add, relu, sigmoid, mse, bce\n",
    "    Each forward function has signature func(cls, w, x) where w is a static parameter like\n",
    "    a parameter matrix or target values y, and x is the input from a previous operation.\n",
    "\n",
    "    Forward operations record their inputs and function on the tape. During the backward()\n",
    "    call, the backward function is computed for that given operation and its inputs.\n",
    "\n",
    "    Backward operations update the gradient of w if w.requires_grad = True\n",
    "    \"\"\"\n",
    "    tape = []\n",
    "    \n",
    "    @classmethod\n",
    "    def matmul(cls, A, x):\n",
    "        cls.tape.append((A, x, \"matmul\"))\n",
    "        return A @ x\n",
    "    \n",
    "    @classmethod\n",
    "    def add(cls, b, x):\n",
    "        cls.tape.append((b, x, \"add\"))\n",
    "        return b + x\n",
    "    \n",
    "    @classmethod\n",
    "    def relu(cls, x):\n",
    "        cls.tape.append((None, x, \"relu\"))\n",
    "        return np.maximum(x, 0)\n",
    "    \n",
    "    @classmethod\n",
    "    def sigmoid(cls, x):\n",
    "        cls.tape.append((None, x, \"sigmoid\"))\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "        \n",
    "    @classmethod\n",
    "    def mse(cls, y, y_hat):\n",
    "        cls.tape.append((y, y_hat, \"mse\"))\n",
    "        return 0.5 * np.sum((y - y_hat)**2)\n",
    "    \n",
    "    # TODO: implement Binary Cross Entropy\n",
    "\n",
    "    @staticmethod\n",
    "    def _matmul_backward(A, x, delta_out):\n",
    "        grad_A = np.outer(delta_out, x)\n",
    "        delta_in = A.T @ delta_out\n",
    "        if A.requires_grad:\n",
    "            A.grad += grad_A\n",
    "        return delta_in\n",
    "\n",
    "    @staticmethod\n",
    "    def _add_backward(b, x, delta_out):\n",
    "        grad_b = np.ones_like(b) * delta_out\n",
    "        delta_in = np.ones_like(x) * delta_out\n",
    "        if b.requires_grad:\n",
    "            b.grad += grad_b\n",
    "        return delta_in\n",
    "    \n",
    "    @staticmethod\n",
    "    def _relu_backward(x, delta_out):\n",
    "        grad = np.ones_like(x)\n",
    "        grad[x <= 0] = 0\n",
    "        delta_in = grad * delta_out\n",
    "        return delta_in\n",
    "\n",
    "    @staticmethod\n",
    "    def _sigmoid_backward(x, delta_out):\n",
    "        sigma = 1 / (1 + np.exp(-x))\n",
    "        delta_in = sigma * (1 - sigma) * delta_out\n",
    "        return delta_in\n",
    "    \n",
    "    @staticmethod\n",
    "    def _mse_backward(y, y_hat, delta_out=1):\n",
    "        return delta_out * (y_hat - y)\n",
    "    \n",
    "    # TODO: Impement BCE backwards\n",
    "\n",
    "    @classmethod\n",
    "    def _op_backward(cls, w, x, op, delta_out):\n",
    "        if op == \"matmul\":\n",
    "            return cls._matmul_backward(w, x, delta_out)\n",
    "        elif op == \"add\":\n",
    "            return cls._add_backward(w, x, delta_out)\n",
    "        elif op == \"relu\":\n",
    "            return cls._relu_backward(x, delta_out)\n",
    "        elif op == \"sigmoid\":\n",
    "            return cls._sigmoid_backward(x, delta_out)\n",
    "        elif op == \"mse\":\n",
    "            return cls._mse_backward(w, x)\n",
    "        else:\n",
    "            raise ValueError(\"Op must be one of ['matmul', 'add', 'relu', 'mse', 'sigmoid']\")\n",
    "\n",
    "    @classmethod\n",
    "    def backward(cls):\n",
    "        delta = 1\n",
    "        for item in reversed(cls.tape):\n",
    "            delta = cls._op_backward(*item, delta)\n",
    "        # Reset the tape\n",
    "        cls.tape = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense:\n",
    "    \"\"\"\n",
    "    Simple Dense layer y = h(A @ x + b)\n",
    "    Supported activations are 'relu' and 'sigmoid'\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, activation='relu', bias=True):\n",
    "        self.A = adarray(np.random.randn(out_features, in_features))\n",
    "        if bias:\n",
    "            self.b = adarray(np.random.randn(out_features))\n",
    "        else:\n",
    "            # Empty bias in case we don't want to use it\n",
    "            self.b = adarray(np.zeros(out_features), requires_grad=False)\n",
    "        if activation == 'relu':\n",
    "            self.activation = NnOps.relu\n",
    "        elif activation == 'sigmoid':\n",
    "            self.activation = NnOps.sigmoid\n",
    "        else:\n",
    "            self.activation = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = NnOps.matmul(self.A, x)\n",
    "        x = NnOps.add(self.b, x)\n",
    "        if self.activation:\n",
    "            x = self.activation(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    \"\"\"Model with 3 fully connected layers\"\"\"\n",
    "    def __init__(self):\n",
    "        self.fc1 = Dense(2, 10)\n",
    "        self.fc2 = Dense(10, 10)\n",
    "        self.fc3 = Dense(10, 1, activation=None)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1.forward(x)\n",
    "        x = self.fc2.forward(x)\n",
    "        x = self.fc3.forward(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(2)\n",
    "y = np.random.randn(1)\n",
    "\n",
    "net = Model()\n",
    "NnOps.tape = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = net.forward(x)\n",
    "loss = NnOps.mse(y, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(adarray([[ 0.89684423,  1.61408643],\n",
       "           [ 0.68717215,  0.19970963],\n",
       "           [ 0.46380819,  0.17467749],\n",
       "           [-0.04398915,  1.54741357],\n",
       "           [ 0.58611085,  0.56754047],\n",
       "           [-0.35836985,  1.14215218],\n",
       "           [-1.47439481,  0.5710566 ],\n",
       "           [-1.20413585,  1.47673708],\n",
       "           [-0.75689184,  1.47745627],\n",
       "           [-1.29848196, -0.22987494]]),\n",
       "  array([-0.69257751, -0.86886552]),\n",
       "  'matmul'),\n",
       " (adarray([-0.16031875, -1.21150219, -1.78456731,  0.77551992,  1.33689699,\n",
       "            1.20708215,  1.28505599, -0.81260678, -0.20667334,  1.59891552]),\n",
       "  adarray([-2.02355819, -0.64944079, -0.47299437, -1.3140284 , -0.89904353,\n",
       "           -0.74417775,  0.5249613 , -0.44912853, -0.75950455,  1.09902981]),\n",
       "  'add'),\n",
       " (None,\n",
       "  adarray([-2.18387694, -1.86094298, -2.25756168, -0.53850849,  0.43785345,\n",
       "            0.4629044 ,  1.81001729, -1.26173531, -0.96617789,  2.69794534]),\n",
       "  'relu'),\n",
       " (adarray([[ 0.5517421 , -0.88792723,  0.07990686, -0.59441351,\n",
       "             0.44220442, -0.28476091, -0.84976165, -0.37496838,\n",
       "             1.25089626, -0.53340508],\n",
       "           [ 1.07697134, -0.93686671,  0.67937413,  0.18907554,\n",
       "            -0.09107536,  2.2469147 ,  1.62462665,  0.69464582,\n",
       "             0.49074142, -0.67759375],\n",
       "           [-1.95830433, -0.43163107,  1.21218769, -0.56016693,\n",
       "             1.54650972,  1.35664555, -0.14563649,  1.21003774,\n",
       "             0.69436368, -0.19532416],\n",
       "           [-1.33039191,  1.26244948, -2.05761366,  1.14807923,\n",
       "            -0.90626104,  0.31711026,  2.02877591, -0.07979661,\n",
       "            -1.03262628, -1.20801119],\n",
       "           [ 1.08308497,  0.77059918,  0.80916582, -0.29014271,\n",
       "            -0.47424119, -0.19820691, -2.19589666,  0.84403195,\n",
       "            -1.60454439, -0.16713383],\n",
       "           [ 0.48819375, -0.11017163,  0.30165222, -1.15221334,\n",
       "             1.2734604 ,  0.21745433,  1.1898398 ,  0.20654079,\n",
       "            -0.29195131, -1.22476362],\n",
       "           [-1.32766616, -0.89609424, -0.91310529, -0.94597877,\n",
       "            -0.69392807, -1.82755298, -0.62154145,  0.53454796,\n",
       "             1.7592957 , -0.92177331],\n",
       "           [-0.51067428, -0.66536693, -0.0316361 ,  1.95756938,\n",
       "            -0.55660602, -0.48811683,  0.56627313, -1.16326618,\n",
       "            -0.70307893, -0.72009547],\n",
       "           [-0.487609  , -1.04900579, -0.62933169, -0.77258526,\n",
       "            -0.86308353,  0.43822661,  0.61281652,  1.76343667,\n",
       "             0.34884869, -0.47964424],\n",
       "           [ 0.48639746,  1.54475968,  0.81821022, -0.42887729,\n",
       "             0.03844364,  1.72780904, -0.52012532, -0.4881838 ,\n",
       "             0.06259336,  0.42682371]]),\n",
       "  adarray([0.        , 0.        , 0.        , 0.        , 0.43785345,\n",
       "           0.4629044 , 1.81001729, 0.        , 0.        , 2.69794534]),\n",
       "  'matmul'),\n",
       " (adarray([ 0.78019194, -1.59950473,  0.9435405 , -0.28946427, -0.44762435,\n",
       "            1.5018052 ,  0.32216302,  0.63191665,  0.40997062, -0.83570484]),\n",
       "  adarray([-2.91537739,  2.11272049,  0.51456335,  0.16295353, -4.72492786,\n",
       "           -0.49246509, -4.76171588, -1.38747736, -0.35989253,  1.02675431]),\n",
       "  'add'),\n",
       " (None,\n",
       "  adarray([-2.13518546,  0.51321576,  1.45810384, -0.12651074, -5.17255221,\n",
       "            1.00934011, -4.43955286, -0.75556071,  0.05007809,  0.19104947]),\n",
       "  'relu'),\n",
       " (adarray([[ 0.90210313, -0.05473954, -0.21565227, -0.34374316,\n",
       "             0.0625358 , -0.05665335,  0.81161955, -0.30861906,\n",
       "             0.71912611,  1.07483823]]),\n",
       "  adarray([0.        , 0.51321576, 1.45810384, 0.        , 0.        ,\n",
       "           1.00934011, 0.        , 0.        , 0.05007809, 0.19104947]),\n",
       "  'matmul'),\n",
       " (adarray([-1.18184982]), adarray([-0.15835935]), 'add'),\n",
       " (array([1.37928745]), adarray([-1.34020917]), 'mse')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NnOps.tape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "NnOps.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ],\n",
       "       [-1.84581915, -2.31565218],\n",
       "       [ 3.28546842,  4.12174839],\n",
       "       [-0.45822958, -0.5748669 ],\n",
       "       [ 0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ],\n",
       "       [ 0.49429953,  0.62011806]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.fc1.A.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare with Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(2)\n",
    "\n",
    "A = np.random.randn(10, 2)\n",
    "b = np.random.randn(10)\n",
    "\n",
    "C = np.random.randn(10, 10)\n",
    "d = np.random.randn(10)\n",
    "\n",
    "E = np.random.randn(1, 10)\n",
    "f = np.random.randn(1)\n",
    "\n",
    "y = np.random.randn(1)\n",
    "\n",
    "def mse_torch(y_hat, y):\n",
    "    return 0.5 * torch.sum((y_hat - y) ** 2)\n",
    "\n",
    "def mse_numpy(y_hat, y):\n",
    "    return 0.5 * np.sum((y_hat - y) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "At = torch.from_numpy(A)\n",
    "At.requires_grad = True\n",
    "bt = torch.from_numpy(b)\n",
    "bt.requires_grad = True\n",
    "Ct = torch.from_numpy(C)\n",
    "Ct.requires_grad = True\n",
    "dt = torch.from_numpy(d)\n",
    "dt.requires_grad = True\n",
    "Et = torch.from_numpy(E)\n",
    "Et.requires_grad = True\n",
    "ft=torch.from_numpy(f)\n",
    "ft.requires_grad = True\n",
    "\n",
    "xt = torch.from_numpy(x)\n",
    "yt = torch.from_numpy(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_torch(xt):\n",
    "    yt = At @ xt + bt\n",
    "    yt = F.relu(yt)\n",
    "    yt = Ct @ yt + dt\n",
    "    yt = F.relu(yt)\n",
    "    yt = Et @ yt + ft\n",
    "    return yt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = mse_torch(model_torch(xt), yt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.1852, dtype=torch.float64, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.backward()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.9969e-02,  4.1153e+00],\n",
       "        [-0.0000e+00,  0.0000e+00],\n",
       "        [-0.0000e+00,  0.0000e+00],\n",
       "        [-9.7304e-03,  2.0053e+00],\n",
       "        [-1.7851e-02,  3.6789e+00],\n",
       "        [ 8.3460e-03, -1.7200e+00],\n",
       "        [-0.0000e+00,  0.0000e+00],\n",
       "        [ 1.2122e-02, -2.4982e+00],\n",
       "        [ 2.8846e-03, -5.9448e-01],\n",
       "        [-0.0000e+00,  0.0000e+00]], dtype=torch.float64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "At.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, A, b, C, d, E, f):\n",
    "        self.fc1 = Dense(2, 10)\n",
    "        self.fc1.A = adarray(A)\n",
    "        self.fc1.b = adarray(b)\n",
    "        \n",
    "        self.fc2 = Dense(10, 10)\n",
    "        self.fc2.A = adarray(C)\n",
    "        self.fc2.b = adarray(d)\n",
    "\n",
    "        self.fc3 = Dense(10, 1, activation=None)\n",
    "        self.fc3.A = adarray(E)\n",
    "        self.fc3.b = adarray(f)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1.forward(x)\n",
    "        x = self.fc2.forward(x)\n",
    "        x = self.fc3.forward(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Model(A, b, C, d, E, f)\n",
    "NnOps.tape = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = net.forward(x)\n",
    "loss = NnOps.mse(y, y_hat)\n",
    "NnOps.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-1.99685085e-02,  4.11526842e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00],\n",
       "        [-9.73041804e-03,  2.00532164e+00],\n",
       "        [-1.78512891e-02,  3.67893507e+00],\n",
       "        [ 8.34603404e-03, -1.72001681e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00],\n",
       "        [ 1.21220679e-02, -2.49821178e+00],\n",
       "        [ 2.88458191e-03, -5.94477492e-01],\n",
       "        [ 0.00000000e+00,  0.00000000e+00]]),\n",
       " tensor([[-1.9969e-02,  4.1153e+00],\n",
       "         [-0.0000e+00,  0.0000e+00],\n",
       "         [-0.0000e+00,  0.0000e+00],\n",
       "         [-9.7304e-03,  2.0053e+00],\n",
       "         [-1.7851e-02,  3.6789e+00],\n",
       "         [ 8.3460e-03, -1.7200e+00],\n",
       "         [-0.0000e+00,  0.0000e+00],\n",
       "         [ 1.2122e-02, -2.4982e+00],\n",
       "         [ 2.8846e-03, -5.9448e-01],\n",
       "         [-0.0000e+00,  0.0000e+00]], dtype=torch.float64))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.fc1.A.grad, At.grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('datasci')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fddaefb328894158b465f763a80d93613a8dda1c2f29f2bb5673421f61ac7a4f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
